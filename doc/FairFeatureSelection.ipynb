{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fairness-Aware Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMPAS Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data \n",
    "import copy\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "fp = '../data/compas-scores-two-years.csv'\n",
    "compas_df = pd.read_csv(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_compas_dataset(compas_df): \n",
    "    #Drop Missing values and subset on columns needed\n",
    "    compas_df.dropna()\n",
    "    compas_subset = compas_df[[\"sex\",\"age\",\"age_cat\",\"race\",\"priors_count\",\"c_charge_degree\",\"c_jail_in\", \"c_jail_out\",'two_year_recid']]\n",
    "    compas_subset[\"two_year_recid\"] = compas_subset[\"two_year_recid\"].apply(lambda x: -1 if x==0 else 1)\n",
    "    \n",
    "    #Only select Caucasian/African American, encode to 0/1\n",
    "    compas_subset = compas_subset[(compas_subset[\"race\"]=='Caucasian') |(compas_subset[\"race\"]=='African-American') ]\n",
    "    compas_subset[\"race_cat\"] = compas_subset[\"race\"].apply(lambda x: 1 if x == \"Caucasian\" else 0)\n",
    "    compas_subset = compas_subset.drop(columns = \"race\")\n",
    "    \n",
    "    #Encode gender to 0/1\n",
    "    compas_subset[\"gender_cat\"] = compas_subset[\"sex\"].apply(lambda x: 1 if x == \"Female\" else 0)\n",
    "    compas_subset = compas_subset.drop(columns = \"sex\")\n",
    "    \n",
    "    #Encode charge degree to 0/1\n",
    "    compas_subset[\"charge_cat\"] = compas_subset[\"c_charge_degree\"].apply(lambda x: 1 if x == \"F\" else 0)\n",
    "    compas_subset = compas_subset.drop(columns = \"c_charge_degree\")\n",
    "    \n",
    "    #Calculate length of stay from jail out - jail in \n",
    "    compas_subset[\"length_stay\"] = pd.to_datetime(compas_subset[\"c_jail_out\"]) - pd.to_datetime(compas_subset['c_jail_in'])\n",
    "    compas_subset[\"length_stay\"] = compas_subset[\"length_stay\"].apply(lambda x: x.days)\n",
    "    compas_subset = compas_subset.drop(columns = [\"c_jail_in\",\"c_jail_out\"])\n",
    "    compas_subset['length_stay'] = compas_subset[\"length_stay\"].apply(lambda x: 0 if x <= 7 else x)\n",
    "    compas_subset['length_stay'] = compas_subset[\"length_stay\"].apply(lambda x: 1 if 7< x <= 90 else x)\n",
    "    compas_subset['length_stay'] = compas_subset[\"length_stay\"].apply(lambda x: 2 if x > 90 else x)\n",
    "    \n",
    "    #Categorize priors count into 3 categories \n",
    "    compas_subset[\"priors_count\"] = compas_subset[\"priors_count\"].apply(lambda x: 0 if x==0 else x)\n",
    "    compas_subset[\"priors_count\"] = compas_subset[\"priors_count\"].apply(lambda x: 1 if (1<=x<=3) else x)\n",
    "    compas_subset[\"priors_count\"] = compas_subset[\"priors_count\"].apply(lambda x: 2 if x>3 else x)\n",
    "    \n",
    "    # Include age as categorical variable \n",
    "    compas_subset = compas_subset.drop(columns = [\"age_cat\"])\n",
    "    \n",
    "    compas_subset = compas_subset.dropna()\n",
    "    y_label = compas_subset[\"two_year_recid\"]\n",
    "    protected_attribute = compas_subset[\"race_cat\"]\n",
    "    df = compas_subset.drop(columns=[\"two_year_recid\",\"race_cat\"])\n",
    "\n",
    "    y_label, protected_attr, df = shuffle(y_label, protected_attribute, df, random_state = 0)\n",
    "\n",
    "    return y_label.to_numpy(), protected_attr.to_numpy(), df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_label, protected_attr, X =  process_compas_dataset(compas_df)\n",
    "\n",
    "train_index = int(len(X)*6./7.)\n",
    "x_train, y_train, race_train = X[:train_index], y_label[:train_index], protected_attr[:train_index]\n",
    "x_test, y_test, race_test = X[train_index:], y_label[train_index:],protected_attr[train_index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we implement the routines described in \"Information Theoretic Measures for Fairness-aware Feature Selection.\" On the training data, we calculate Shapley coefficients for each of our features capturing effects on both accuracy and discrimation on our protected group (i.e. race)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This cell contains utility functions called in the proceeding cells.\"\"\"\n",
    "\n",
    "def get_uniq_vals_in_arr(arr):\n",
    "    \"\"\"Returns unique values in array.\n",
    "    \n",
    "    :param arr (np.array) n * m matrix\n",
    "    :return (list) uniq_vals[i] contains unique values of ith column in arr\n",
    "    \"\"\"\n",
    "    uniq_vals = []\n",
    "    for id_col in range(arr.shape[1]):\n",
    "        uniq_vals.append(np.unique(arr[:, id_col]).tolist())\n",
    "    return uniq_vals\n",
    "\n",
    "\n",
    "def powerset(seq):\n",
    "    \"\"\"\n",
    "    Returns all the subsets of this set. This is a generator.\n",
    "    \"\"\"\n",
    "    if len(seq) <= 1:\n",
    "        yield seq\n",
    "        yield []\n",
    "    else:\n",
    "        for item in powerset(seq[1:]):\n",
    "            yield [seq[0]]+item\n",
    "            yield item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This cell contains code for all the routines needed to calculate the Shapley coefficients.\"\"\"\n",
    "\n",
    "def get_info_coef(left, right):\n",
    "    # Both arrays NEED same number of rows\n",
    "    assert left.shape[0] == right.shape[0]\n",
    "    num_rows = left.shape[0]\n",
    "    num_left_cols = left.shape[1]\n",
    "        \n",
    "    concat_mat = np.concatenate((left, right), axis=1)\n",
    "    concat_uniq_vals = get_uniq_vals_in_arr(concat_mat)\n",
    "    concat_combos = list(itertools.product(*concat_uniq_vals))\n",
    "    p_sum = 0\n",
    "    for vec in concat_combos:\n",
    "        p_r1_r2 = len(np.where((concat_mat == vec).all(axis=1))[0]) / num_rows\n",
    "        p_r1 = len(np.where((left == vec[:num_left_cols]).all(axis=1))[0]) / num_rows\n",
    "        p_r2 = len(np.where((right == vec[num_left_cols:]).all(axis=1))[0]) / num_rows\n",
    "        \n",
    "        if p_r1_r2 == 0 or p_r1 == 0 or p_r2 == 0:\n",
    "            p_iter = 0\n",
    "        else:\n",
    "            p_iter = p_r1_r2 * np.log(p_r1_r2 / p_r1) / p_r1\n",
    "        p_sum += np.abs(p_iter)\n",
    "    return p_sum\n",
    "\n",
    "\n",
    "def get_conditional_info_coef(left, right, conditional): \n",
    "    assert (left.shape[0] == right.shape[0]) and (left.shape[0] == conditional.shape[0])\n",
    "    num_rows = left.shape[0]\n",
    "    num_left_cols = left.shape[1]\n",
    "    num_right_cols = right.shape[1]\n",
    "\n",
    "    right_concat_mat = np.concatenate((right, conditional), axis=1)    \n",
    "    concat_mat = np.concatenate((left, right_concat_mat), axis=1)\n",
    "    concat_uniq_vals = get_uniq_vals_in_arr(concat_mat)\n",
    "    concat_combos = list(itertools.product(*concat_uniq_vals))\n",
    "    p_sum = 0\n",
    "    for vec in concat_combos:\n",
    "        p_r1_r2 = len(np.where((concat_mat == vec).all(axis=1))[0]) / num_rows\n",
    "        p_r1 = len(np.where((left == vec[:num_left_cols]).all(axis=1))[0]) / num_rows\n",
    "        p_r2 = len(np.where((concat_mat[:, num_left_cols: -num_right_cols] == vec[num_left_cols: -num_right_cols]).all(axis=1))[0]) / num_rows\n",
    "        \n",
    "        try:\n",
    "            p_r1_given_r3 = len(np.where((concat_mat[:, :num_left_cols] == vec[:num_left_cols]).all(axis=1) & (concat_mat[:, -num_right_cols:] == vec[-num_right_cols:]).all(axis=1))[0]) / len(np.where((concat_mat[:, -num_right_cols:] == vec[-num_right_cols:]).all(axis=1))[0])\n",
    "        except ZeroDivisionError:\n",
    "            p_r1_given_r3 = 0\n",
    "        \n",
    "        if p_r1_r2 == 0 or p_r1 == 0 or p_r2 == 0 or p_r1_given_r3 == 0:\n",
    "            p_iter = 0\n",
    "        else:\n",
    "            p_iter = p_r1_r2 * np.log(p_r1_r2 / p_r2) / p_r1_given_r3\n",
    "        p_sum += np.abs(p_iter)\n",
    "    return p_sum\n",
    "\n",
    "\n",
    "def get_acc_coef(y, x_s, x_s_c, protected_attr):\n",
    "    conditional = np.concatenate((x_s_c, protected_attr), axis=1)\n",
    "    return get_conditional_info_coef(y, x_s, conditional)\n",
    "\n",
    "\n",
    "def get_disc_coef(y, x_s, protected_attr):\n",
    "    x_s_a = np.concatenate((x_s, protected_attr), axis=1)\n",
    "    return get_info_coef(y, x_s_a) * get_info_coef(x_s, protected_attr) * get_conditional_info_coef(x_s, protected_attr, y)\n",
    "\n",
    "\n",
    "def get_shapley_acc_i(y, x, protected_attr, i):\n",
    "    \"\"\"Returns Shapley coeffecient of ith feature in x.\"\"\"\n",
    "    \n",
    "    num_features = x.shape[1]\n",
    "    lst_idx = list(range(num_features))\n",
    "    lst_idx.pop(i)\n",
    "    power_set = [x for x in powerset(lst_idx) if len(x) > 0]\n",
    "    \n",
    "    shapley = 0\n",
    "    for set_idx in power_set:\n",
    "        coef = math.factorial(len(set_idx)) * math.factorial(num_features - len(set_idx) - 1) / math.factorial(num_features)\n",
    "        \n",
    "        # Calculate v(T U {i})\n",
    "        idx_xs_incl = copy.copy(set_idx)\n",
    "        idx_xs_incl.append(i)\n",
    "        idx_xsc_incl = list(set(list(range(num_features))).difference(set(idx_xs_incl)))\n",
    "        acc_incl = get_acc_coef(y.reshape(-1, 1), x[:, idx_xs_incl], x[:, idx_xsc_incl], protected_attr.reshape(-1, 1))\n",
    "        \n",
    "        # Calculate v(T)\n",
    "        idx_xsc_excl = list(range(num_features))\n",
    "        idx_xsc_excl.pop(i)\n",
    "        idx_xsc_excl = list(set(idx_xsc_excl).difference(set(set_idx)))\n",
    "        acc_excl = get_acc_coef(y.reshape(-1, 1), x[:, set_idx], x[:, idx_xsc_excl], protected_attr.reshape(-1, 1))\n",
    "        \n",
    "        marginal = acc_incl - acc_excl\n",
    "        shapley = shapley + coef * marginal\n",
    "    return shapley\n",
    "\n",
    "\n",
    "def get_shapley_disc_i(y, x, protected_attr, i):\n",
    "    \"\"\"Returns Shapley coeffecient of ith feature in x.\"\"\"\n",
    "    \n",
    "    num_features = x.shape[1]\n",
    "    lst_idx = list(range(num_features))\n",
    "    lst_idx.pop(i)\n",
    "    power_set = [x for x in powerset(lst_idx) if len(x) > 0]\n",
    "    \n",
    "    shapley = 0\n",
    "    for set_idx in power_set:\n",
    "        coef = math.factorial(len(set_idx)) * math.factorial(num_features - len(set_idx) - 1) / math.factorial(num_features)\n",
    "        \n",
    "        # Calculate v_D(T U {i})\n",
    "        idx_xs_incl = copy.copy(set_idx)\n",
    "        idx_xs_incl.append(i)\n",
    "        disc_incl = get_disc_coef(y.reshape(-1, 1), x[:, idx_xs_incl], protected_attr.reshape(-1, 1))\n",
    "        \n",
    "        # Calculate v_D(T)\n",
    "        disc_excl = get_disc_coef(y.reshape(-1, 1), x[:, set_idx], protected_attr.reshape(-1, 1))\n",
    "        \n",
    "        marginal = disc_incl - disc_excl\n",
    "        shapley = shapley + coef * marginal\n",
    "    return shapley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Shapley disc, acc coefs for each feature over training data\n",
    "shap_acc = []\n",
    "shap_disc = []\n",
    "for i in range(5):\n",
    "    acc_i = get_shapley_acc_i(y_train, x_train, race_train, i)\n",
    "    disc_i = get_shapley_disc_i(y_train, x_train, race_train, i)\n",
    "    \n",
    "    shap_acc.append(acc_i)\n",
    "    shap_disc.append(disc_i)\n",
    "\n",
    "# Build Shapley output\n",
    "feature_names = [\"Prior Count\", \"Gender\", \"Charge Degree\", \"Length of Stay\", \"Age (Categorical)\"]\n",
    "shapley_df = pd.DataFrame(list(zip(feature_names, shap_acc, shap_disc)),\n",
    "                          columns=[\"Feature\", \"Shapley (Accuracy)\", \"Shapley (Discrimination)\"])\n",
    "shapley_df = shapley_df.sort_values(by=[\"Shapley (Discrimination)\"], ascending=[False]).reset_index(0, True)\n",
    "shapley_df.to_csv(\"../output/compas-data-shapley-table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.2E' % x)\n",
    "shapley_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that 'Prior Count' has the sharpest affect on both discrimination and accuracy, so eliminating it can prove problematic for a classifier. However, a feature such as 'Age (Categorical)' is relatively discriminatory but eliminating it would not seriously reduce accuracy from our results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we build an SVM model that predicts whether a user will or will not recidivate given the aforementioned features. We calculate accuracy as well as calibration, the difference between accuracy amongst the groups. Finally, we build submodels which eliminate each feature iteratively and calculate both metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = []\n",
    "test_cal = []\n",
    "\n",
    "# Build model for overall data inclusive of all features\n",
    "svm = SVC(kernel=\"linear\").fit(x_train, y_train)\n",
    "idx_race_1, idx_race_0  = np.where(race_test == 1)[0], np.where(race_test == 0)[0]\n",
    "test_acc.append(svm.score(x_test, y_test))\n",
    "test_cal.append(svm.score(x_test[idx_race_1], y_test[idx_race_1]) - svm.score(x_test[idx_race_0], y_test[idx_race_0]))\n",
    "\n",
    "# Eliminate one feature at a time build model\n",
    "for id_feature in range(x_train.shape[1]):\n",
    "    idxs = list(range(x_train.shape[1]))\n",
    "    idxs.pop(id_feature)\n",
    "    x_train_mod = x_train[:, idxs]\n",
    "    x_test_mod = x_test[:, idxs]\n",
    "    \n",
    "    svm = SVC(kernel=\"linear\").fit(x_train_mod, y_train)\n",
    "    acc = svm.score(x_test_mod, y_test)\n",
    "    cal = svm.score(x_test_mod[idx_race_1], y_test[idx_race_1]) - svm.score(x_test_mod[idx_race_0], y_test[idx_race_0])\n",
    "    \n",
    "    test_acc.append(acc)\n",
    "    test_cal.append(cal)\n",
    "    \n",
    "\n",
    "index_names = [\"None\", \"Prior Count\", \"Gender\", \"Charge Degree\", \"Length of Stay\", \"Age (Categorical)\"]\n",
    "test_acc = [x * 100 for x in test_acc]\n",
    "test_cal = [x * 100 for x in test_cal]\n",
    "results = pd.DataFrame(list(zip(index_names, test_acc, test_cal)),\n",
    "                          columns=[\"Eliminating Feature\", \"Accuracy (%)\", \"Calibration (%)\"])\n",
    "results[\"Delta (%)\"] = 2.00 - results[\"Calibration (%)\"]\n",
    "results.to_csv(\"../output/compas-data-test-results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When evaluating the results, we must compare against the baseline calibration score of 2%.\n",
    "\n",
    "Clearly, our results differ somewhat from our pretraining procedures. For example, we show that eliminating 'Prior Count' should result in the strongest drop in discrimation but the results suggest that actually eliminating 'Gender' yields the greatest benefit to discrimation. However, our FFS process showed that dropping 'Charge Degree' is unnecessary and the results prove that as removing it from the training set results in a significantly more discriminatory classifier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
